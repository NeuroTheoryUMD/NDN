

<!DOCTYPE html>
<!--[if IE 8]><html class="no-js lt-ie9" lang="en" > <![endif]-->
<!--[if gt IE 8]><!--> <html class="no-js" lang="en" > <!--<![endif]-->
<head>
  <meta charset="utf-8">
  
  <meta name="viewport" content="width=device-width, initial-scale=1.0">
  
  <title>FFnetwork.ffnetwork &mdash; Neural Deep Network 2.0 documentation</title>
  

  
  
  
  

  

  
  
    

  

  
  
    <link rel="stylesheet" href="../../_static/css/theme.css" type="text/css" />
  

  

  
        <link rel="index" title="Index"
              href="../../genindex.html"/>
        <link rel="search" title="Search" href="../../search.html"/>
    <link rel="top" title="Neural Deep Network 2.0 documentation" href="../../index.html"/>
        <link rel="up" title="Module code" href="../index.html"/> 

  
  <script src="../../_static/js/modernizr.min.js"></script>

</head>

<body class="wy-body-for-nav" role="document">

   
  <div class="wy-grid-for-nav">

    
    <nav data-toggle="wy-nav-shift" class="wy-nav-side">
      <div class="wy-side-scroll">
        <div class="wy-side-nav-search">
          

          
            <a href="../../index.html" class="icon icon-home"> Neural Deep Network
          

          
          </a>

          
            
            
              <div class="version">
                2.0
              </div>
            
          

          
<div role="search">
  <form id="rtd-search-form" class="wy-form" action="../../search.html" method="get">
    <input type="text" name="q" placeholder="Search docs" />
    <input type="hidden" name="check_keywords" value="yes" />
    <input type="hidden" name="area" value="default" />
  </form>
</div>

          
        </div>

        <div class="wy-menu wy-menu-vertical" data-spy="affix" role="navigation" aria-label="main navigation">
          
            
            
              
            
            
              <!-- Local TOC -->
              <div class="local-toc"></div>
            
          
        </div>
      </div>
    </nav>

    <section data-toggle="wy-nav-shift" class="wy-nav-content-wrap">

      
      <nav class="wy-nav-top" role="navigation" aria-label="top navigation">
        
          <i data-toggle="wy-nav-top" class="fa fa-bars"></i>
          <a href="../../index.html">Neural Deep Network</a>
        
      </nav>


      
      <div class="wy-nav-content">
        <div class="rst-content">
          















<div role="navigation" aria-label="breadcrumbs navigation">

  <ul class="wy-breadcrumbs">
    
      <li><a href="../../index.html">Docs</a> &raquo;</li>
        
          <li><a href="../index.html">Module code</a> &raquo;</li>
        
      <li>FFnetwork.ffnetwork</li>
    
    
      <li class="wy-breadcrumbs-aside">
        
            
        
      </li>
    
  </ul>

  
  <hr/>
</div>
          <div role="main" class="document" itemscope="itemscope" itemtype="http://schema.org/Article">
           <div itemprop="articleBody">
            
  <h1>Source code for FFnetwork.ffnetwork</h1><div class="highlight"><pre>
<span></span><span class="sd">&quot;&quot;&quot;Basic network-building tools&quot;&quot;&quot;</span>

<span class="kn">from</span> <span class="nn">__future__</span> <span class="k">import</span> <span class="n">print_function</span>
<span class="kn">from</span> <span class="nn">__future__</span> <span class="k">import</span> <span class="n">division</span>

<span class="kn">import</span> <span class="nn">tensorflow</span> <span class="k">as</span> <span class="nn">tf</span>
<span class="kn">import</span> <span class="nn">numpy</span> <span class="k">as</span> <span class="nn">np</span>
<span class="kn">from</span> <span class="nn">.layer</span> <span class="k">import</span> <span class="n">Layer</span>
<span class="kn">from</span> <span class="nn">.layer</span> <span class="k">import</span> <span class="n">convLayer</span>
<span class="kn">from</span> <span class="nn">.layer</span> <span class="k">import</span> <span class="n">sepLayer</span>


<div class="viewcode-block" id="FFNetwork"><a class="viewcode-back" href="../../source/FFnetwork.html#FFnetwork.ffnetwork.FFNetwork">[docs]</a><span class="k">class</span> <span class="nc">FFNetwork</span><span class="p">(</span><span class="nb">object</span><span class="p">):</span>
    <span class="sd">&quot;&quot;&quot;Implementation of simple fully connected feedforward neural network</span>

<span class="sd">    Attributes:</span>
<span class="sd">        scope (str): name scope for network</span>
<span class="sd">        layers (list of `Layer` objects): layers of network</span>
<span class="sd">        num_layers (int): number of layers in network (not including input)</span>
<span class="sd">        log (bool): use tf summary writers in layer activations</span>

<span class="sd">    &quot;&quot;&quot;</span>

<div class="viewcode-block" id="FFNetwork.__init__"><a class="viewcode-back" href="../../source/FFnetwork.html#FFnetwork.ffnetwork.FFNetwork.__init__">[docs]</a>    <span class="k">def</span> <span class="nf">__init__</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span>
                 <span class="n">scope</span><span class="o">=</span><span class="kc">None</span><span class="p">,</span>
                 <span class="n">input_dims</span><span class="o">=</span><span class="kc">None</span><span class="p">,</span>
                 <span class="n">params_dict</span><span class="o">=</span><span class="kc">None</span><span class="p">):</span>
        <span class="sd">&quot;&quot;&quot;Constructor for FFNetwork class</span>

<span class="sd">        Args:</span>
<span class="sd">            scope (str): name scope for network</span>
<span class="sd">            params_dict (dict): contains parameters about details of FFnetwork:</span>
<span class="sd">            -&gt; layer_sizes (list of ints): list of layer sizes, including input </span>
<span class="sd">                and output. All arguments (input size) can be up to a </span>
<span class="sd">                3-dimensional list. REQUIRED (NO DEFAULT)</span>
<span class="sd">            -&gt; num_inh: list or single number denoting number of inhibitory units in each</span>
<span class="sd">                layer. This specifies the output of that number of units multiplied by -1</span>
<span class="sd">                DEFAULT = 0 (and having any single value will be used for all layers)</span>
<span class="sd">            -&gt; activation_funcs (str or list of strs, optional): pointwise</span>
<span class="sd">                function for each layer; replicated if a single element. </span>
<span class="sd">                DEFAULT = &#39;relu&#39;. See Layer class for other options.</span>
<span class="sd">            -&gt; pos_constraints (bool or list of bools, optional): constrains all weights to be positive</span>
<span class="sd">                DEFAULTS = False.</span>
<span class="sd">            -&gt; reg_initializer (dict): a list of dictionaries: one for each layer. Within the</span>
<span class="sd">                dictionary, reg_type/vals as key-value pairs.</span>
<span class="sd">                DEFAULT = None</span>
<span class="sd">            -&gt; weights_initializer (str or list of strs, optional): initializer</span>
<span class="sd">                for the weights in each layer; replicated if a single element.</span>
<span class="sd">                DEFAULT = &#39;trunc_normal&#39;. See Layer class for other options.</span>
<span class="sd">            -&gt; biases_initializer (str or list of strs, optional): initializer for</span>
<span class="sd">                the biases in each layer; replicated if a single element.</span>
<span class="sd">                DEFAULT = &#39;zeros&#39;. See Layer class for other options.</span>
<span class="sd">            -&gt; log_activations (bool, optional): True to use tf.summary on layer activations</span>
<span class="sd">                DEFAULT = False</span>

<span class="sd">        Raises:</span>
<span class="sd">            TypeError: If `scope` is not specified</span>
<span class="sd">            TypeError: If `inputs` is not specified</span>
<span class="sd">            TypeError: If `layer_sizes` is not specified</span>
<span class="sd">            ValueError: If `activation_funcs` is not a properly-sized list</span>
<span class="sd">            ValueError: If `weights_initializer` is not a properly-sized list</span>
<span class="sd">            ValueError: If `biases_initializer` is not a properly-sized list</span>

<span class="sd">        &quot;&quot;&quot;</span>

        <span class="c1"># check for required inputs</span>
        <span class="k">if</span> <span class="n">scope</span> <span class="ow">is</span> <span class="kc">None</span><span class="p">:</span>
            <span class="k">raise</span> <span class="ne">TypeError</span><span class="p">(</span><span class="s1">&#39;Must specify network scope&#39;</span><span class="p">)</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">scope</span> <span class="o">=</span> <span class="n">scope</span>

        <span class="k">if</span> <span class="n">params_dict</span> <span class="ow">is</span> <span class="kc">None</span><span class="p">:</span>
            <span class="k">raise</span> <span class="ne">TypeError</span><span class="p">(</span><span class="s1">&#39;Must specify parameters dictionary.&#39;</span><span class="p">)</span>

        <span class="k">if</span> <span class="n">input_dims</span> <span class="ow">is</span> <span class="kc">None</span><span class="p">:</span>
            <span class="n">input_dims</span> <span class="o">=</span> <span class="n">params_dict</span><span class="p">[</span><span class="s1">&#39;input_dims&#39;</span><span class="p">]</span>
            <span class="k">if</span> <span class="n">params_dict</span><span class="p">[</span><span class="s1">&#39;input_dims&#39;</span><span class="p">]</span> <span class="ow">is</span> <span class="kc">None</span><span class="p">:</span>
                <span class="k">raise</span> <span class="ne">TypeError</span><span class="p">(</span><span class="s1">&#39;Must specify input dimensions.&#39;</span><span class="p">)</span>
        <span class="c1"># Format input dims (or check formatting)</span>
        <span class="k">if</span> <span class="ow">not</span> <span class="nb">isinstance</span><span class="p">(</span> <span class="n">input_dims</span><span class="p">,</span> <span class="nb">list</span><span class="p">):</span>
            <span class="n">input_dims</span> <span class="o">=</span> <span class="p">[</span><span class="mi">1</span><span class="p">,</span> <span class="n">input_dims</span><span class="p">,</span> <span class="mi">1</span><span class="p">]</span>
        <span class="k">else</span><span class="p">:</span>
            <span class="k">while</span> <span class="nb">len</span><span class="p">(</span><span class="n">input_dims</span><span class="p">)</span> <span class="o">&lt;</span> <span class="mi">3</span><span class="p">:</span>
                <span class="n">input_dims</span><span class="o">.</span><span class="n">append</span><span class="p">(</span><span class="mi">1</span><span class="p">)</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">input_dims</span> <span class="o">=</span> <span class="n">input_dims</span>

        <span class="c1"># Check information in params_dict and set defaults</span>
        <span class="k">if</span> <span class="s1">&#39;layer_sizes&#39;</span> <span class="ow">not</span> <span class="ow">in</span> <span class="n">params_dict</span><span class="p">:</span>
            <span class="k">raise</span> <span class="ne">TypeError</span><span class="p">(</span><span class="s1">&#39;Must specify layer_sizes.&#39;</span><span class="p">)</span>

        <span class="bp">self</span><span class="o">.</span><span class="n">num_layers</span> <span class="o">=</span> <span class="nb">len</span><span class="p">(</span><span class="n">params_dict</span><span class="p">[</span><span class="s1">&#39;layer_sizes&#39;</span><span class="p">])</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">layer_types</span> <span class="o">=</span> <span class="n">params_dict</span><span class="p">[</span><span class="s1">&#39;layer_types&#39;</span><span class="p">]</span>

        <span class="k">if</span> <span class="s1">&#39;activation_funcs&#39;</span> <span class="ow">not</span> <span class="ow">in</span> <span class="n">params_dict</span><span class="p">:</span>
            <span class="n">params_dict</span><span class="p">[</span><span class="s1">&#39;activation_funcs&#39;</span><span class="p">]</span> <span class="o">=</span> <span class="s1">&#39;relu&#39;</span>
        <span class="k">if</span> <span class="nb">type</span><span class="p">(</span><span class="n">params_dict</span><span class="p">[</span><span class="s1">&#39;activation_funcs&#39;</span><span class="p">])</span> <span class="ow">is</span> <span class="ow">not</span> <span class="nb">list</span><span class="p">:</span>
            <span class="n">params_dict</span><span class="p">[</span><span class="s1">&#39;activation_funcs&#39;</span><span class="p">]</span> <span class="o">=</span> \
                <span class="p">[</span><span class="n">params_dict</span><span class="p">[</span><span class="s1">&#39;activation_funcs&#39;</span><span class="p">]]</span> <span class="o">*</span> <span class="bp">self</span><span class="o">.</span><span class="n">num_layers</span>
        <span class="k">elif</span> <span class="nb">len</span><span class="p">(</span><span class="n">params_dict</span><span class="p">[</span><span class="s1">&#39;activation_funcs&#39;</span><span class="p">])</span> <span class="o">!=</span> <span class="bp">self</span><span class="o">.</span><span class="n">num_layers</span><span class="p">:</span>
            <span class="k">raise</span> <span class="ne">ValueError</span><span class="p">(</span><span class="s1">&#39;Invalid number of activation_funcs&#39;</span><span class="p">)</span>

        <span class="k">if</span> <span class="s1">&#39;weights_initializers&#39;</span> <span class="ow">not</span> <span class="ow">in</span> <span class="n">params_dict</span><span class="p">:</span>
            <span class="n">params_dict</span><span class="p">[</span><span class="s1">&#39;weights_initializers&#39;</span><span class="p">]</span> <span class="o">=</span> <span class="s1">&#39;trunc_normal&#39;</span>
        <span class="k">if</span> <span class="nb">type</span><span class="p">(</span><span class="n">params_dict</span><span class="p">[</span><span class="s1">&#39;weights_initializers&#39;</span><span class="p">])</span> <span class="ow">is</span> <span class="ow">not</span> <span class="nb">list</span><span class="p">:</span>
            <span class="n">params_dict</span><span class="p">[</span><span class="s1">&#39;weights_initializers&#39;</span><span class="p">]</span> <span class="o">=</span> \
                <span class="p">[</span><span class="n">params_dict</span><span class="p">[</span><span class="s1">&#39;weights_initializers&#39;</span><span class="p">]]</span> <span class="o">*</span> <span class="bp">self</span><span class="o">.</span><span class="n">num_layers</span>
        <span class="k">elif</span> <span class="nb">len</span><span class="p">(</span><span class="n">params_dict</span><span class="p">[</span><span class="s1">&#39;weights_initializers&#39;</span><span class="p">])</span> <span class="o">!=</span> <span class="bp">self</span><span class="o">.</span><span class="n">num_layers</span><span class="p">:</span>
            <span class="k">raise</span> <span class="ne">ValueError</span><span class="p">(</span><span class="s1">&#39;Invalid number of weights_initializer&#39;</span><span class="p">)</span>

        <span class="k">if</span> <span class="s1">&#39;biases_initializers&#39;</span> <span class="ow">not</span> <span class="ow">in</span> <span class="n">params_dict</span><span class="p">:</span>
            <span class="n">params_dict</span><span class="p">[</span><span class="s1">&#39;biases_initializers&#39;</span><span class="p">]</span> <span class="o">=</span> <span class="s1">&#39;zeros&#39;</span>
        <span class="k">if</span> <span class="nb">type</span><span class="p">(</span><span class="n">params_dict</span><span class="p">[</span><span class="s1">&#39;biases_initializers&#39;</span><span class="p">])</span> <span class="ow">is</span> <span class="ow">not</span> <span class="nb">list</span><span class="p">:</span>
            <span class="n">params_dict</span><span class="p">[</span><span class="s1">&#39;biases_initializers&#39;</span><span class="p">]</span> <span class="o">=</span> \
                <span class="p">[</span><span class="n">params_dict</span><span class="p">[</span><span class="s1">&#39;biases_initializers&#39;</span><span class="p">]]</span> <span class="o">*</span> <span class="bp">self</span><span class="o">.</span><span class="n">num_layers</span>
        <span class="k">elif</span> <span class="nb">len</span><span class="p">(</span><span class="n">params_dict</span><span class="p">[</span><span class="s1">&#39;biases_initializers&#39;</span><span class="p">])</span> <span class="o">!=</span> <span class="bp">self</span><span class="o">.</span><span class="n">num_layers</span><span class="p">:</span>
            <span class="k">raise</span> <span class="ne">ValueError</span><span class="p">(</span><span class="s1">&#39;Invalid number of biases_initializer&#39;</span><span class="p">)</span>

        <span class="k">if</span> <span class="s1">&#39;num_inh&#39;</span> <span class="ow">not</span> <span class="ow">in</span> <span class="n">params_dict</span><span class="p">:</span>
            <span class="n">params_dict</span><span class="p">[</span><span class="s1">&#39;num_inh&#39;</span><span class="p">]</span> <span class="o">=</span> <span class="mi">0</span>
        <span class="k">if</span> <span class="nb">type</span><span class="p">(</span><span class="n">params_dict</span><span class="p">[</span><span class="s1">&#39;num_inh&#39;</span><span class="p">])</span> <span class="ow">is</span> <span class="ow">not</span> <span class="nb">list</span><span class="p">:</span>
            <span class="n">params_dict</span><span class="p">[</span><span class="s1">&#39;num_inh&#39;</span><span class="p">]</span> <span class="o">=</span> <span class="p">[</span><span class="n">params_dict</span><span class="p">[</span><span class="s1">&#39;num_inh&#39;</span><span class="p">]]</span> <span class="o">*</span> <span class="bp">self</span><span class="o">.</span><span class="n">num_layers</span>
        <span class="k">elif</span> <span class="nb">len</span><span class="p">(</span><span class="n">params_dict</span><span class="p">[</span><span class="s1">&#39;num_inh&#39;</span><span class="p">])</span> <span class="o">!=</span> <span class="bp">self</span><span class="o">.</span><span class="n">num_layers</span><span class="p">:</span>
            <span class="k">raise</span> <span class="ne">ValueError</span><span class="p">(</span><span class="s1">&#39;Invalid number of num_inh&#39;</span><span class="p">)</span>

        <span class="k">if</span> <span class="s1">&#39;pos_constraints&#39;</span> <span class="ow">not</span> <span class="ow">in</span> <span class="n">params_dict</span><span class="p">:</span>
            <span class="n">params_dict</span><span class="p">[</span><span class="s1">&#39;pos_constraints&#39;</span><span class="p">]</span> <span class="o">=</span> <span class="kc">False</span>
        <span class="k">if</span> <span class="nb">type</span><span class="p">(</span><span class="n">params_dict</span><span class="p">[</span><span class="s1">&#39;pos_constraints&#39;</span><span class="p">])</span> <span class="ow">is</span> <span class="ow">not</span> <span class="nb">list</span><span class="p">:</span>
            <span class="n">params_dict</span><span class="p">[</span><span class="s1">&#39;pos_constraints&#39;</span><span class="p">]</span> <span class="o">=</span> \
                <span class="p">[</span><span class="n">params_dict</span><span class="p">[</span><span class="s1">&#39;pos_constraints&#39;</span><span class="p">]]</span> <span class="o">*</span> <span class="bp">self</span><span class="o">.</span><span class="n">num_layers</span>
        <span class="k">elif</span> <span class="nb">len</span><span class="p">(</span><span class="n">params_dict</span><span class="p">[</span><span class="s1">&#39;pos_constraints&#39;</span><span class="p">])</span> <span class="o">!=</span> <span class="bp">self</span><span class="o">.</span><span class="n">num_layers</span><span class="p">:</span>
            <span class="k">raise</span> <span class="ne">ValueError</span><span class="p">(</span><span class="s1">&#39;Invalid number of pos_con&#39;</span><span class="p">)</span>

        <span class="k">if</span> <span class="s1">&#39;log_activations&#39;</span> <span class="ow">not</span> <span class="ow">in</span> <span class="n">params_dict</span><span class="p">:</span>
            <span class="n">params_dict</span><span class="p">[</span><span class="s1">&#39;log_activations&#39;</span><span class="p">]</span> <span class="o">=</span> <span class="kc">False</span>

        <span class="k">if</span> <span class="s1">&#39;reg_initializers&#39;</span> <span class="ow">not</span> <span class="ow">in</span> <span class="n">params_dict</span><span class="p">:</span>
            <span class="n">params_dict</span><span class="p">[</span><span class="s1">&#39;reg_initializers&#39;</span><span class="p">]</span> <span class="o">=</span> <span class="p">[</span><span class="kc">None</span><span class="p">]</span><span class="o">*</span><span class="bp">self</span><span class="o">.</span><span class="n">num_layers</span>

        <span class="c1"># Define network</span>
        <span class="k">with</span> <span class="n">tf</span><span class="o">.</span><span class="n">name_scope</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">scope</span><span class="p">):</span>
            <span class="bp">self</span><span class="o">.</span><span class="n">_define_network</span><span class="p">(</span><span class="n">params_dict</span><span class="p">)</span>

        <span class="k">if</span> <span class="n">params_dict</span><span class="p">[</span><span class="s1">&#39;log_activations&#39;</span><span class="p">]:</span>
            <span class="bp">self</span><span class="o">.</span><span class="n">log</span> <span class="o">=</span> <span class="kc">True</span>
        <span class="k">else</span><span class="p">:</span>
            <span class="bp">self</span><span class="o">.</span><span class="n">log</span> <span class="o">=</span> <span class="kc">False</span></div>
    <span class="c1"># END FFNetwork.__init__</span>

    <span class="k">def</span> <span class="nf">_define_network</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">network_params</span><span class="p">):</span>

        <span class="n">layer_sizes</span> <span class="o">=</span> <span class="p">[</span><span class="bp">self</span><span class="o">.</span><span class="n">input_dims</span><span class="p">]</span> <span class="o">+</span> <span class="n">network_params</span><span class="p">[</span><span class="s1">&#39;layer_sizes&#39;</span><span class="p">]</span>

        <span class="bp">self</span><span class="o">.</span><span class="n">layers</span> <span class="o">=</span> <span class="p">[]</span>
        <span class="k">for</span> <span class="n">nn</span> <span class="ow">in</span> <span class="nb">range</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">num_layers</span><span class="p">):</span>

            <span class="k">if</span> <span class="bp">self</span><span class="o">.</span><span class="n">layer_types</span><span class="p">[</span><span class="n">nn</span><span class="p">]</span> <span class="ow">is</span> <span class="s1">&#39;normal&#39;</span><span class="p">:</span>

                <span class="bp">self</span><span class="o">.</span><span class="n">layers</span><span class="o">.</span><span class="n">append</span><span class="p">(</span>
                    <span class="n">Layer</span><span class="p">(</span> <span class="n">scope</span><span class="o">=</span><span class="s1">&#39;layer_</span><span class="si">%i</span><span class="s1">&#39;</span> <span class="o">%</span> <span class="n">nn</span><span class="p">,</span>
                           <span class="n">input_dims</span><span class="o">=</span><span class="n">layer_sizes</span><span class="p">[</span><span class="n">nn</span><span class="p">],</span>
                           <span class="n">output_dims</span><span class="o">=</span><span class="n">layer_sizes</span><span class="p">[</span><span class="n">nn</span><span class="o">+</span><span class="mi">1</span><span class="p">],</span>
                           <span class="n">activation_func</span><span class="o">=</span><span class="n">network_params</span><span class="p">[</span><span class="s1">&#39;activation_funcs&#39;</span><span class="p">][</span><span class="n">nn</span><span class="p">],</span>
                           <span class="n">normalize_weights</span><span class="o">=</span><span class="n">network_params</span><span class="p">[</span><span class="s1">&#39;normalize_weights&#39;</span><span class="p">][</span><span class="n">nn</span><span class="p">],</span>
                           <span class="n">weights_initializer</span><span class="o">=</span><span class="n">network_params</span><span class="p">[</span><span class="s1">&#39;weights_initializers&#39;</span><span class="p">][</span><span class="n">nn</span><span class="p">],</span>
                           <span class="n">biases_initializer</span><span class="o">=</span><span class="n">network_params</span><span class="p">[</span><span class="s1">&#39;biases_initializers&#39;</span><span class="p">][</span><span class="n">nn</span><span class="p">],</span>
                           <span class="n">reg_initializer</span><span class="o">=</span><span class="n">network_params</span><span class="p">[</span><span class="s1">&#39;reg_initializers&#39;</span><span class="p">][</span><span class="n">nn</span><span class="p">],</span>
                           <span class="n">num_inh</span><span class="o">=</span><span class="n">network_params</span><span class="p">[</span><span class="s1">&#39;num_inh&#39;</span><span class="p">][</span><span class="n">nn</span><span class="p">],</span>
                           <span class="n">pos_constraint</span><span class="o">=</span><span class="n">network_params</span><span class="p">[</span><span class="s1">&#39;pos_constraints&#39;</span><span class="p">][</span><span class="n">nn</span><span class="p">],</span>
                           <span class="n">log_activations</span><span class="o">=</span><span class="n">network_params</span><span class="p">[</span><span class="s1">&#39;log_activations&#39;</span><span class="p">]</span> <span class="p">)</span> <span class="p">)</span>

            <span class="k">elif</span> <span class="bp">self</span><span class="o">.</span><span class="n">layer_types</span><span class="p">[</span><span class="n">nn</span><span class="p">]</span> <span class="ow">is</span> <span class="s1">&#39;sep&#39;</span><span class="p">:</span>

                <span class="bp">self</span><span class="o">.</span><span class="n">layers</span><span class="o">.</span><span class="n">append</span><span class="p">(</span>
                    <span class="n">sepLayer</span><span class="p">(</span> <span class="n">scope</span><span class="o">=</span><span class="s1">&#39;layer_</span><span class="si">%i</span><span class="s1">&#39;</span> <span class="o">%</span> <span class="n">nn</span><span class="p">,</span>
                              <span class="n">input_dims</span><span class="o">=</span><span class="n">layer_sizes</span><span class="p">[</span><span class="n">nn</span><span class="p">],</span>
                              <span class="n">output_dims</span><span class="o">=</span><span class="n">layer_sizes</span><span class="p">[</span><span class="n">nn</span><span class="o">+</span><span class="mi">1</span><span class="p">],</span>
                              <span class="n">activation_func</span><span class="o">=</span><span class="n">network_params</span><span class="p">[</span><span class="s1">&#39;activation_funcs&#39;</span><span class="p">][</span><span class="n">nn</span><span class="p">],</span>
                              <span class="n">normalize_weights</span><span class="o">=</span><span class="n">network_params</span><span class="p">[</span><span class="s1">&#39;normalize_weights&#39;</span><span class="p">][</span><span class="n">nn</span><span class="p">],</span>
                              <span class="n">weights_initializer</span><span class="o">=</span><span class="n">network_params</span><span class="p">[</span><span class="s1">&#39;weights_initializers&#39;</span><span class="p">][</span><span class="n">nn</span><span class="p">],</span>
                              <span class="n">biases_initializer</span><span class="o">=</span><span class="n">network_params</span><span class="p">[</span><span class="s1">&#39;biases_initializers&#39;</span><span class="p">][</span><span class="n">nn</span><span class="p">],</span>
                              <span class="n">reg_initializer</span><span class="o">=</span><span class="n">network_params</span><span class="p">[</span><span class="s1">&#39;reg_initializers&#39;</span><span class="p">][</span><span class="n">nn</span><span class="p">],</span>
                              <span class="n">num_inh</span><span class="o">=</span><span class="n">network_params</span><span class="p">[</span><span class="s1">&#39;num_inh&#39;</span><span class="p">][</span><span class="n">nn</span><span class="p">],</span>
                              <span class="n">pos_constraint</span><span class="o">=</span><span class="n">network_params</span><span class="p">[</span><span class="s1">&#39;pos_constraints&#39;</span><span class="p">][</span><span class="n">nn</span><span class="p">],</span>
                              <span class="n">log_activations</span><span class="o">=</span><span class="n">network_params</span><span class="p">[</span><span class="s1">&#39;log_activations&#39;</span><span class="p">]</span> <span class="p">)</span> <span class="p">)</span>


            <span class="k">elif</span> <span class="bp">self</span><span class="o">.</span><span class="n">layer_types</span><span class="p">[</span><span class="n">nn</span><span class="p">]</span> <span class="ow">is</span> <span class="s1">&#39;conv&#39;</span><span class="p">:</span>

                <span class="k">if</span> <span class="n">network_params</span><span class="p">[</span><span class="s1">&#39;conv_filter_widths&#39;</span><span class="p">][</span><span class="n">nn</span><span class="p">]</span> <span class="ow">is</span> <span class="kc">None</span><span class="p">:</span>
                    <span class="n">conv_filter_size</span> <span class="o">=</span> <span class="n">layer_sizes</span><span class="p">[</span><span class="n">nn</span><span class="o">+</span><span class="mi">1</span><span class="p">]</span>
                <span class="k">else</span><span class="p">:</span>
                    <span class="n">conv_filter_size</span> <span class="o">=</span> <span class="p">[</span><span class="n">layer_sizes</span><span class="p">[</span><span class="n">nn</span><span class="p">][</span><span class="mi">0</span><span class="p">],</span> <span class="n">network_params</span><span class="p">[</span><span class="s1">&#39;conv_filter_widths&#39;</span><span class="p">][</span><span class="n">nn</span><span class="p">],</span> <span class="mi">1</span><span class="p">]</span>
                    <span class="k">if</span> <span class="n">layer_sizes</span><span class="p">[</span><span class="n">nn</span><span class="p">][</span><span class="mi">2</span><span class="p">]</span> <span class="o">&gt;</span> <span class="mi">1</span><span class="p">:</span>
                        <span class="n">conv_filter_size</span><span class="p">[</span><span class="mi">2</span><span class="p">]</span> <span class="o">=</span> <span class="n">network_params</span><span class="p">[</span><span class="s1">&#39;conv_filter_widths&#39;</span><span class="p">][</span><span class="n">nn</span><span class="p">]</span>

                <span class="bp">self</span><span class="o">.</span><span class="n">layers</span><span class="o">.</span><span class="n">append</span><span class="p">(</span>
                    <span class="n">convLayer</span><span class="p">(</span> <span class="n">scope</span><span class="o">=</span><span class="s1">&#39;conv_layer_</span><span class="si">%i</span><span class="s1">&#39;</span> <span class="o">%</span> <span class="n">nn</span><span class="p">,</span>
                               <span class="n">input_dims</span><span class="o">=</span><span class="n">layer_sizes</span><span class="p">[</span><span class="n">nn</span><span class="p">],</span>
                               <span class="n">num_filters</span><span class="o">=</span><span class="n">layer_sizes</span><span class="p">[</span><span class="n">nn</span><span class="o">+</span><span class="mi">1</span><span class="p">],</span>
                               <span class="n">filter_dims</span><span class="o">=</span><span class="n">conv_filter_size</span><span class="p">,</span>
                               <span class="n">shift_spacing</span><span class="o">=</span><span class="n">network_params</span><span class="p">[</span><span class="s1">&#39;shift_spacing&#39;</span><span class="p">][</span><span class="n">nn</span><span class="p">],</span>
                               <span class="n">activation_func</span><span class="o">=</span><span class="n">network_params</span><span class="p">[</span><span class="s1">&#39;activation_funcs&#39;</span><span class="p">][</span><span class="n">nn</span><span class="p">],</span>
                               <span class="n">normalize_weights</span><span class="o">=</span><span class="n">network_params</span><span class="p">[</span><span class="s1">&#39;normalize_weights&#39;</span><span class="p">][</span><span class="n">nn</span><span class="p">],</span>
                               <span class="n">weights_initializer</span><span class="o">=</span><span class="n">network_params</span><span class="p">[</span><span class="s1">&#39;weights_initializers&#39;</span><span class="p">][</span><span class="n">nn</span><span class="p">],</span>
                               <span class="n">biases_initializer</span><span class="o">=</span><span class="n">network_params</span><span class="p">[</span><span class="s1">&#39;biases_initializers&#39;</span><span class="p">][</span><span class="n">nn</span><span class="p">],</span>
                               <span class="n">reg_initializer</span><span class="o">=</span><span class="n">network_params</span><span class="p">[</span><span class="s1">&#39;reg_initializers&#39;</span><span class="p">][</span><span class="n">nn</span><span class="p">],</span>
                               <span class="n">num_inh</span><span class="o">=</span><span class="n">network_params</span><span class="p">[</span><span class="s1">&#39;num_inh&#39;</span><span class="p">][</span><span class="n">nn</span><span class="p">],</span>
                               <span class="n">pos_constraint</span><span class="o">=</span><span class="n">network_params</span><span class="p">[</span><span class="s1">&#39;pos_constraints&#39;</span><span class="p">][</span><span class="n">nn</span><span class="p">],</span>
                               <span class="n">log_activations</span><span class="o">=</span><span class="n">network_params</span><span class="p">[</span><span class="s1">&#39;log_activations&#39;</span><span class="p">])</span> <span class="p">)</span>

                <span class="c1"># Modify output size to take into account shifts</span>
                <span class="k">if</span> <span class="n">nn</span> <span class="o">&lt;</span> <span class="bp">self</span><span class="o">.</span><span class="n">num_layers</span><span class="p">:</span>
                    <span class="n">layer_sizes</span><span class="p">[</span><span class="n">nn</span><span class="o">+</span><span class="mi">1</span><span class="p">]</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">layers</span><span class="p">[</span><span class="n">nn</span><span class="p">]</span><span class="o">.</span><span class="n">output_dims</span>

            <span class="k">else</span><span class="p">:</span>
                <span class="k">raise</span> <span class="ne">TypeError</span><span class="p">(</span><span class="s1">&#39;Layer type </span><span class="si">%i</span><span class="s1"> not defined.&#39;</span> <span class="o">%</span> <span class="n">nn</span> <span class="p">)</span>

            <span class="c1"># END FFNetwork._define_network</span>

    <span class="k">def</span> <span class="nf">_build_fit_variable_list</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">fit_parameter_list</span><span class="p">):</span>
        <span class="sd">&quot;&quot;&quot;makes a list of variables of this network that will be fit given argument&quot;&quot;&quot;</span>

        <span class="n">var_list</span> <span class="o">=</span> <span class="p">[]</span>
        <span class="k">for</span> <span class="n">layer</span> <span class="ow">in</span> <span class="nb">range</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">num_layers</span><span class="p">):</span>
            <span class="k">if</span> <span class="n">fit_parameter_list</span><span class="p">[</span><span class="n">layer</span><span class="p">][</span><span class="s2">&quot;weights&quot;</span><span class="p">]:</span>
                <span class="n">var_list</span><span class="o">.</span><span class="n">append</span><span class="p">(</span> <span class="bp">self</span><span class="o">.</span><span class="n">layers</span><span class="p">[</span><span class="n">layer</span><span class="p">]</span><span class="o">.</span><span class="n">weights_var</span> <span class="p">)</span>
            <span class="k">if</span> <span class="n">fit_parameter_list</span><span class="p">[</span><span class="n">layer</span><span class="p">][</span><span class="s2">&quot;biases&quot;</span><span class="p">]:</span>
                <span class="n">var_list</span><span class="o">.</span><span class="n">append</span><span class="p">(</span> <span class="bp">self</span><span class="o">.</span><span class="n">layers</span><span class="p">[</span><span class="n">layer</span><span class="p">]</span><span class="o">.</span><span class="n">biases_var</span> <span class="p">)</span>
        <span class="k">return</span> <span class="n">var_list</span>
    <span class="c1"># END FFNetwork._build_fit_variable_list</span>

<div class="viewcode-block" id="FFNetwork.build_graph"><a class="viewcode-back" href="../../source/FFnetwork.html#FFnetwork.ffnetwork.FFNetwork.build_graph">[docs]</a>    <span class="k">def</span> <span class="nf">build_graph</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">inputs</span><span class="p">,</span> <span class="n">params_dict</span><span class="o">=</span><span class="kc">None</span><span class="p">):</span>

        <span class="k">with</span> <span class="n">tf</span><span class="o">.</span><span class="n">name_scope</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">scope</span><span class="p">):</span>
            <span class="k">for</span> <span class="n">layer</span> <span class="ow">in</span> <span class="nb">range</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">num_layers</span><span class="p">):</span>
                <span class="bp">self</span><span class="o">.</span><span class="n">layers</span><span class="p">[</span><span class="n">layer</span><span class="p">]</span><span class="o">.</span><span class="n">build_graph</span><span class="p">(</span><span class="n">inputs</span><span class="p">,</span> <span class="n">params_dict</span><span class="p">)</span>
                <span class="n">inputs</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">layers</span><span class="p">[</span><span class="n">layer</span><span class="p">]</span><span class="o">.</span><span class="n">outputs</span></div>
    <span class="c1"># END FFNetwork._build_graph</span>

<div class="viewcode-block" id="FFNetwork.assign_model_params"><a class="viewcode-back" href="../../source/FFnetwork.html#FFnetwork.ffnetwork.FFNetwork.assign_model_params">[docs]</a>    <span class="k">def</span> <span class="nf">assign_model_params</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">sess</span><span class="p">):</span>
        <span class="sd">&quot;&quot;&quot;Read weights/biases in numpy arrays into tf Variables&quot;&quot;&quot;</span>
        <span class="k">with</span> <span class="n">tf</span><span class="o">.</span><span class="n">name_scope</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">scope</span><span class="p">):</span>
            <span class="k">for</span> <span class="n">layer</span> <span class="ow">in</span> <span class="nb">range</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">num_layers</span><span class="p">):</span>
                <span class="bp">self</span><span class="o">.</span><span class="n">layers</span><span class="p">[</span><span class="n">layer</span><span class="p">]</span><span class="o">.</span><span class="n">assign_layer_params</span><span class="p">(</span><span class="n">sess</span><span class="p">)</span></div>

<div class="viewcode-block" id="FFNetwork.write_model_params"><a class="viewcode-back" href="../../source/FFnetwork.html#FFnetwork.ffnetwork.FFNetwork.write_model_params">[docs]</a>    <span class="k">def</span> <span class="nf">write_model_params</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">sess</span><span class="p">):</span>
        <span class="sd">&quot;&quot;&quot;Write weights/biases in tf Variables to numpy arrays&quot;&quot;&quot;</span>
        <span class="k">for</span> <span class="n">layer</span> <span class="ow">in</span> <span class="nb">range</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">num_layers</span><span class="p">):</span>
            <span class="bp">self</span><span class="o">.</span><span class="n">layers</span><span class="p">[</span><span class="n">layer</span><span class="p">]</span><span class="o">.</span><span class="n">write_layer_params</span><span class="p">(</span><span class="n">sess</span><span class="p">)</span></div>

<div class="viewcode-block" id="FFNetwork.assign_reg_vals"><a class="viewcode-back" href="../../source/FFnetwork.html#FFnetwork.ffnetwork.FFNetwork.assign_reg_vals">[docs]</a>    <span class="k">def</span> <span class="nf">assign_reg_vals</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">sess</span><span class="p">):</span>
        <span class="sd">&quot;&quot;&quot;Update default tf Graph with new regularization penalties&quot;&quot;&quot;</span>
        <span class="k">with</span> <span class="n">tf</span><span class="o">.</span><span class="n">name_scope</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">scope</span><span class="p">):</span>
            <span class="k">for</span> <span class="n">layer</span> <span class="ow">in</span> <span class="nb">range</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">num_layers</span><span class="p">):</span>
                <span class="bp">self</span><span class="o">.</span><span class="n">layers</span><span class="p">[</span><span class="n">layer</span><span class="p">]</span><span class="o">.</span><span class="n">assign_reg_vals</span><span class="p">(</span><span class="n">sess</span><span class="p">)</span></div>

<div class="viewcode-block" id="FFNetwork.define_regularization_loss"><a class="viewcode-back" href="../../source/FFnetwork.html#FFnetwork.ffnetwork.FFNetwork.define_regularization_loss">[docs]</a>    <span class="k">def</span> <span class="nf">define_regularization_loss</span><span class="p">(</span><span class="bp">self</span><span class="p">):</span>
        <span class="sd">&quot;&quot;&quot;Build regularization loss portion of default tf graph&quot;&quot;&quot;</span>

        <span class="k">with</span> <span class="n">tf</span><span class="o">.</span><span class="n">name_scope</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">scope</span><span class="p">):</span>
            <span class="c1"># define regularization loss for each layer separately...</span>
            <span class="n">reg_ops</span> <span class="o">=</span> <span class="p">[</span><span class="kc">None</span> <span class="k">for</span> <span class="n">_</span> <span class="ow">in</span> <span class="nb">range</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">num_layers</span><span class="p">)]</span>
            <span class="k">for</span> <span class="n">layer</span> <span class="ow">in</span> <span class="nb">range</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">num_layers</span><span class="p">):</span>
                <span class="n">reg_ops</span><span class="p">[</span><span class="n">layer</span><span class="p">]</span> <span class="o">=</span> \
                    <span class="bp">self</span><span class="o">.</span><span class="n">layers</span><span class="p">[</span><span class="n">layer</span><span class="p">]</span><span class="o">.</span><span class="n">define_regularization_loss</span><span class="p">()</span>
            <span class="c1"># ...then sum over all layers</span>
            <span class="n">reg_loss</span> <span class="o">=</span> <span class="n">tf</span><span class="o">.</span><span class="n">add_n</span><span class="p">(</span><span class="n">reg_ops</span><span class="p">)</span>
        <span class="k">return</span> <span class="n">reg_loss</span></div></div>


<div class="viewcode-block" id="side_network"><a class="viewcode-back" href="../../source/FFnetwork.html#FFnetwork.ffnetwork.side_network">[docs]</a><span class="k">class</span> <span class="nc">side_network</span><span class="p">(</span><span class="n">FFNetwork</span><span class="p">):</span>
    <span class="sd">&quot;&quot;&quot;Implementation of siFFNetwork</span>
<span class="sd">    &quot;&quot;&quot;</span>

<div class="viewcode-block" id="side_network.__init__"><a class="viewcode-back" href="../../source/FFnetwork.html#FFnetwork.ffnetwork.side_network.__init__">[docs]</a>    <span class="k">def</span> <span class="nf">__init__</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">scope</span><span class="o">=</span><span class="kc">None</span><span class="p">,</span> <span class="n">input_network_params</span><span class="o">=</span><span class="kc">None</span><span class="p">,</span> <span class="n">params_dict</span><span class="o">=</span><span class="kc">None</span><span class="p">):</span>
        <span class="sd">&quot;&quot;&quot;Constructor for Network class</span>

<span class="sd">        Args:</span>
<span class="sd">            scope (str): name scope for network</span>
<span class="sd">            input_network_params: parameters of network that is input to this network</span>
<span class="sd">            params_dict (dictionary with the following fields):</span>
<span class="sd">                SI-NETWORK SPECIFIC:</span>
<span class="sd">                -&gt; first_filter_size: size of filters in first layer, if different than input size</span>
<span class="sd">                    DEFAULT = input size</span>
<span class="sd">                -&gt; shift_spacing (int): convolutional &quot;strides&quot; to be passed back into conv2d</span>
<span class="sd">                    DEFAULT = 1</span>
<span class="sd">                -&gt; binocular (boolean): currently doesn&#39;t work</span>
<span class="sd">                    DEFAULT = FALSE</span>

<span class="sd">                INHERITED FFNetwork PARAMS (see FFNetwork documentation):</span>
<span class="sd">                -&gt; layer_sizes (list of ints)</span>
<span class="sd">                -&gt; activation_funcs (str or list of strs, optional)</span>
<span class="sd">                -&gt; weights_initializer (str or list of strs, optional)</span>
<span class="sd">                -&gt; biases_initializer (str or list of strs, optional)</span>
<span class="sd">                -&gt; reg_initializers (list of dicts)</span>
<span class="sd">                -&gt; num_inh (None, int or list of ints, optional)</span>
<span class="sd">                -&gt; pos_constraint (bool or list of bools, optional):</span>
<span class="sd">                -&gt; log_activations (bool, optional)</span>

<span class="sd">        Raises:</span>
<span class="sd">            TypeError: If `scope` is not specified</span>
<span class="sd">            TypeError: If `inputs` is not specified</span>
<span class="sd">            TypeError: If `layer_sizes` is not specified</span>
<span class="sd">        &quot;&quot;&quot;</span>

        <span class="c1"># Determine dimensions of input and pass into regular network initializer</span>
        <span class="n">input_layer_sizes</span> <span class="o">=</span> <span class="n">input_network_params</span><span class="p">[</span><span class="s1">&#39;layer_sizes&#39;</span><span class="p">]</span>

        <span class="nb">super</span><span class="p">(</span><span class="n">side_network</span><span class="p">,</span> <span class="bp">self</span><span class="p">)</span><span class="o">.</span><span class="fm">__init__</span><span class="p">(</span>
            <span class="n">scope</span><span class="o">=</span><span class="n">scope</span><span class="p">,</span>
            <span class="n">input_dims</span> <span class="o">=</span> <span class="p">[</span><span class="nb">len</span><span class="p">(</span><span class="n">input_layer_sizes</span><span class="p">),</span> <span class="nb">max</span><span class="p">(</span><span class="n">input_layer_sizes</span><span class="p">),</span> <span class="mi">1</span><span class="p">],</span>
            <span class="n">params_dict</span><span class="o">=</span><span class="n">params_dict</span><span class="p">)</span>

        <span class="bp">self</span><span class="o">.</span><span class="n">num_units</span> <span class="o">=</span> <span class="n">input_layer_sizes</span></div>
    <span class="c1"># END side_network.__init__</span>

<div class="viewcode-block" id="side_network.build_graph"><a class="viewcode-back" href="../../source/FFnetwork.html#FFnetwork.ffnetwork.side_network.build_graph">[docs]</a>    <span class="k">def</span> <span class="nf">build_graph</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">input_network</span><span class="p">,</span> <span class="n">params_dict</span><span class="o">=</span><span class="kc">None</span><span class="p">):</span>
        <span class="sd">&quot;&quot;&quot;Note this is different from other network build-graphs in that the whole</span>
<span class="sd">        network graph, rather than just a link to its output, so that it can be assembled here&quot;&quot;&quot;</span>

        <span class="n">max_units</span> <span class="o">=</span> <span class="nb">max</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">num_units</span><span class="p">)</span>
        <span class="k">with</span> <span class="n">tf</span><span class="o">.</span><span class="n">name_scope</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">scope</span><span class="p">):</span>

            <span class="c1"># Assemble network-inputs into the first layer</span>
            <span class="k">for</span> <span class="n">input_nn</span> <span class="ow">in</span> <span class="nb">range</span><span class="p">(</span><span class="nb">len</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">num_units</span><span class="p">)):</span>
                <span class="k">if</span> <span class="n">input_nn</span> <span class="o">==</span> <span class="mi">0</span><span class="p">:</span>
                    <span class="n">inputs</span> <span class="o">=</span> <span class="n">input_network</span><span class="o">.</span><span class="n">layers</span><span class="p">[</span><span class="n">input_nn</span><span class="p">]</span><span class="o">.</span><span class="n">outputs</span>
                <span class="k">else</span><span class="p">:</span>
                    <span class="n">inputs</span> <span class="o">=</span> <span class="n">tf</span><span class="o">.</span><span class="n">concat</span><span class="p">(</span> <span class="p">(</span><span class="n">inputs</span><span class="p">,</span> <span class="n">input_network</span><span class="o">.</span><span class="n">layers</span><span class="p">[</span><span class="n">input_nn</span><span class="p">]</span><span class="o">.</span><span class="n">outputs</span><span class="p">),</span> <span class="mi">1</span> <span class="p">)</span>

                <span class="k">if</span> <span class="n">max_units</span><span class="o">-</span><span class="bp">self</span><span class="o">.</span><span class="n">num_units</span><span class="p">[</span><span class="n">input_nn</span><span class="p">]</span> <span class="o">&gt;</span> <span class="mi">0</span><span class="p">:</span>
                    <span class="n">layer_padding</span> <span class="o">=</span> <span class="n">tf</span><span class="o">.</span><span class="n">constant</span><span class="p">([[</span><span class="mi">0</span><span class="p">,</span> <span class="mi">0</span><span class="p">],</span> <span class="p">[</span><span class="mi">0</span><span class="p">,</span> <span class="n">max_units</span><span class="o">-</span><span class="bp">self</span><span class="o">.</span><span class="n">num_units</span><span class="p">[</span><span class="n">input_nn</span><span class="p">]]])</span>
                    <span class="n">inputs</span> <span class="o">=</span> <span class="n">tf</span><span class="o">.</span><span class="n">pad</span><span class="p">(</span> <span class="n">inputs</span><span class="p">,</span> <span class="n">layer_padding</span> <span class="p">)</span>

            <span class="c1"># Now standard graph-build (could just call the parent with inputs)</span>
            <span class="k">for</span> <span class="n">layer</span> <span class="ow">in</span> <span class="nb">range</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">num_layers</span><span class="p">):</span>
                <span class="bp">self</span><span class="o">.</span><span class="n">layers</span><span class="p">[</span><span class="n">layer</span><span class="p">]</span><span class="o">.</span><span class="n">build_graph</span><span class="p">(</span><span class="n">inputs</span><span class="p">,</span> <span class="n">params_dict</span><span class="p">)</span>
                <span class="n">inputs</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">layers</span><span class="p">[</span><span class="n">layer</span><span class="p">]</span><span class="o">.</span><span class="n">outputs</span></div></div>
    <span class="c1"># END side_network.build_graph</span>

</pre></div>

           </div>
           <div class="articleComments">
            
           </div>
          </div>
          <footer>
  

  <hr/>

  <div role="contentinfo">
    <p>
        &copy; Copyright 2018, Dan Butts, Matt Whiteway.

    </p>
  </div>
  Built with <a href="http://sphinx-doc.org/">Sphinx</a> using a <a href="https://github.com/snide/sphinx_rtd_theme">theme</a> provided by <a href="https://readthedocs.org">Read the Docs</a>. 

</footer>

        </div>
      </div>

    </section>

  </div>
  


  

    <script type="text/javascript">
        var DOCUMENTATION_OPTIONS = {
            URL_ROOT:'../../',
            VERSION:'2.0',
            COLLAPSE_INDEX:false,
            FILE_SUFFIX:'.html',
            HAS_SOURCE:  true,
            SOURCELINK_SUFFIX: '.txt'
        };
    </script>
      <script type="text/javascript" src="../../_static/jquery.js"></script>
      <script type="text/javascript" src="../../_static/underscore.js"></script>
      <script type="text/javascript" src="../../_static/doctools.js"></script>
      <script type="text/javascript" src="https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.1/MathJax.js?config=TeX-AMS-MML_HTMLorMML"></script>

  

  
  
    <script type="text/javascript" src="../../_static/js/theme.js"></script>
  

  
  
  <script type="text/javascript">
      jQuery(function () {
          SphinxRtdTheme.StickyNav.enable();
      });
  </script>
   

</body>
</html>